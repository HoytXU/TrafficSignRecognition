{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Level - Traffic Sign Recognition\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. **T1: Dataset Loading** - Load images and extract class labels\n",
    "2. **T2: Preprocessing Methods** - Compare different preprocessing techniques\n",
    "3. **T3: Feature Extraction** - Implement and compare different feature extraction methods\n",
    "4. **T4: Classification Models** - Test different classifiers and find best combinations\n",
    "\n",
    "## Baseline Setup\n",
    "\n",
    "- **Preprocessing**: Resize + Grayscale + Gaussian Blur\n",
    "- **Features**: HOG (Histogram of Oriented Gradients)\n",
    "- **Classifier**: SVM (Support Vector Machine)\n",
    "\n",
    "**Note**: See `concepts.ipynb` for visualizations and concept explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from skimage.color import rgb2hsv\n",
    "from skimage.measure import regionprops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1: Dataset Loading\n",
    "\n",
    "Load images and extract class IDs from filenames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 5998 images\n",
      "✓ 58 unique classes\n",
      "✓ Sample class IDs: ['000', '001', '002', '003', '004', '005', '006', '007', '008', '009']...\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"Load dataset from disk.\"\"\"\n",
    "    # Get project root (go up from expert/ folder)\n",
    "    current_dir = os.getcwd()\n",
    "    if 'expert' in current_dir:\n",
    "        project_root = os.path.dirname(current_dir)\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    \n",
    "    dataset_path = os.path.join(project_root, \"datasets\", \"dataset1\") + os.sep\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    image_files = glob.glob(dataset_path + '*.png', recursive=True)\n",
    "    \n",
    "    for i in image_files:\n",
    "        filename = os.path.basename(i)\n",
    "        class_id = filename[:3]\n",
    "        y.append(class_id)\n",
    "        \n",
    "        img = cv2.imread(i)\n",
    "        if img is not None:\n",
    "            X.append(img)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_dataset()\n",
    "print(f\"✓ Loaded {len(X)} images\")\n",
    "print(f\"✓ {len(set(y))} unique classes\")\n",
    "print(f\"✓ Sample class IDs: {sorted(set(y))[:10]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2: Preprocessing Methods\n",
    "\n",
    "Four preprocessing techniques:\n",
    "\n",
    "1. **Simple**: Resize + Grayscale\n",
    "2. **Blur**: Resize + Grayscale + Gaussian Blur (reduces noise)\n",
    "3. **Histogram Eq**: Equalize histograms + Resize + Grayscale (improves contrast)\n",
    "4. **Advanced**: Resize + Grayscale + Blur + Histogram Eq + Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def preprocessing_simple(X):\n",
    "    \"\"\"Simple: Resize + Grayscale\"\"\"\n",
    "    X_processed = []\n",
    "    for x in X:\n",
    "        temp_x = cv2.resize(x, (48, 48))\n",
    "        temp_x = cv2.cvtColor(temp_x, cv2.COLOR_BGR2GRAY)\n",
    "        X_processed.append(temp_x)\n",
    "    return X_processed\n",
    "\n",
    "def preprocessing_blur(X):\n",
    "    \"\"\"Blur: Resize + Grayscale + Gaussian Blur\"\"\"\n",
    "    X_processed = []\n",
    "    for x in X:\n",
    "        temp_x = cv2.resize(x, (48, 48))\n",
    "        temp_x = cv2.cvtColor(temp_x, cv2.COLOR_BGR2GRAY)\n",
    "        temp_x = cv2.GaussianBlur(temp_x, (3, 3), 0)\n",
    "        X_processed.append(temp_x)\n",
    "    return X_processed\n",
    "\n",
    "def preprocessing_histogram_eq(X):\n",
    "    \"\"\"Histogram Equalization: Equalize each channel + Resize + Grayscale\"\"\"\n",
    "    X_processed = []\n",
    "    for x in X:\n",
    "        b, g, r = cv2.split(x)\n",
    "        bH = cv2.equalizeHist(b)\n",
    "        gH = cv2.equalizeHist(g)\n",
    "        rH = cv2.equalizeHist(r)\n",
    "        result = cv2.merge((bH, gH, rH))\n",
    "        result = cv2.resize(result, (48, 48))\n",
    "        result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "        X_processed.append(result)\n",
    "    return X_processed\n",
    "\n",
    "def preprocessing_advanced(X):\n",
    "    \"\"\"Advanced: Resize + Grayscale + Blur + Histogram Equalization + Normalization\"\"\"\n",
    "    X_processed = []\n",
    "    for x in X:\n",
    "        temp_x = cv2.resize(x, (48, 48))\n",
    "        temp_x = cv2.cvtColor(temp_x, cv2.COLOR_BGR2GRAY)\n",
    "        temp_x = cv2.GaussianBlur(temp_x, (3, 3), 0)\n",
    "        temp_x = cv2.equalizeHist(temp_x)\n",
    "        temp_x = temp_x.astype(np.float32) / 255.0\n",
    "        X_processed.append(temp_x)\n",
    "    return X_processed\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3: Feature Extraction\n",
    "\n",
    "Implement different feature extraction methods and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "def feature_HOG(X_processed):\n",
    "    \"\"\"HOG: Histogram of Oriented Gradients\"\"\"\n",
    "    X_features = []\n",
    "    for x in X_processed:\n",
    "        x_feature = hog(x, orientations=8, pixels_per_cell=(10, 10),\n",
    "                        cells_per_block=(1, 1), visualize=False)\n",
    "        X_features.append(x_feature)\n",
    "    return np.array(X_features)\n",
    "\n",
    "def feature_LBP(X_processed):\n",
    "    \"\"\"LBP: Local Binary Pattern\"\"\"\n",
    "    X_features = []\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    for x in X_processed:\n",
    "        x_feature = local_binary_pattern(x, n_points, radius)\n",
    "        x_feature = x_feature.reshape(-1)\n",
    "        X_features.append(x_feature)\n",
    "    return np.array(X_features)\n",
    "\n",
    "def feature_pyramid(X_processed):\n",
    "    \"\"\"Feature Pyramid: Multi-scale Laplacian pyramid\"\"\"\n",
    "    num_layers = 3\n",
    "    X_features = []\n",
    "    for x in X_processed:\n",
    "        gaussian_pyr = [x]\n",
    "        image = x\n",
    "        for i in range(1, num_layers):\n",
    "            image = cv2.pyrDown(image)\n",
    "            gaussian_pyr.append(image)\n",
    "        \n",
    "        laplacian_pyr = [gaussian_pyr[num_layers - 1]]\n",
    "        for i in range(num_layers - 1, 0, -1):\n",
    "            expanded = cv2.pyrUp(gaussian_pyr[i])\n",
    "            laplacian = cv2.subtract(gaussian_pyr[i - 1], expanded)\n",
    "            laplacian_pyr.append(laplacian)\n",
    "        \n",
    "        flattened_arrays = [arr.flatten() for arr in laplacian_pyr]\n",
    "        x_feature = np.concatenate(flattened_arrays)\n",
    "        X_features.append(x_feature)\n",
    "    return np.array(X_features)\n",
    "\n",
    "def feature_FFT(X_processed):\n",
    "    \"\"\"FFT: Frequency domain magnitude spectrum\"\"\"\n",
    "    X_features = []\n",
    "    for x in X_processed:\n",
    "        f = np.fft.fft2(x)\n",
    "        f_shift = np.fft.fftshift(f)\n",
    "        magnitude_spectrum = 20 * np.log(np.abs(f_shift) + 1)\n",
    "        magnitude_spectrum = magnitude_spectrum.reshape(-1)\n",
    "        X_features.append(magnitude_spectrum)\n",
    "    return np.array(X_features)\n",
    "\n",
    "def feature_HuMoments(X_processed):\n",
    "    \"\"\"Hu Moments: 7 invariant shape moments\"\"\"\n",
    "    X_features = []\n",
    "    for x in X_processed:\n",
    "        ret, binary = cv2.threshold(x, 127, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) == 0:\n",
    "            hu_moments = np.zeros(7)\n",
    "        else:\n",
    "            moments = cv2.moments(contours[0])\n",
    "            hu_moments = cv2.HuMoments(moments).reshape(-1)\n",
    "        X_features.append(hu_moments)\n",
    "    return np.array(X_features)\n",
    "\n",
    "def feature_multi(X, X_processed):\n",
    "    \"\"\"Multi-feature: HOG + Color (HSV) + Shape features\"\"\"\n",
    "    X_features = []\n",
    "    for x, x_processed in zip(X, X_processed):\n",
    "        # HOG\n",
    "        hog_feature = hog(x_processed, orientations=8, pixels_per_cell=(10, 10),\n",
    "                          cells_per_block=(1, 1), visualize=False)\n",
    "        \n",
    "        # Color features (HSV)\n",
    "        hsv_image = rgb2hsv(x)\n",
    "        hue_hist = np.histogram(hsv_image[:, :, 0], bins=8, range=(0, 1))[0]\n",
    "        sat_hist = np.histogram(hsv_image[:, :, 1], bins=8, range=(0, 1))[0]\n",
    "        val_hist = np.histogram(hsv_image[:, :, 2], bins=8, range=(0, 1))[0]\n",
    "        \n",
    "        # Shape features\n",
    "        label_img = np.uint8(x_processed > 0)\n",
    "        props = regionprops(label_img)[0]\n",
    "        shape_features = [props.area, props.perimeter, props.eccentricity]\n",
    "        \n",
    "        # Combine\n",
    "        x_features = np.concatenate((hog_feature, hue_hist, sat_hist, val_hist, shape_features))\n",
    "        X_features.append(x_features)\n",
    "    return np.array(X_features)\n",
    "\n",
    "print(\"✓ Feature extraction functions defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T4: Classification Models\n",
    "\n",
    "Test different classifiers and find the best preprocessing + feature + classifier combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4.1: Compare Preprocessing Methods\n",
    "\n",
    "**Goal**: Compare preprocessing techniques.\n",
    "\n",
    "**Setup**: HOG features + SVM classifier, vary preprocessing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "T4.1: Compare Preprocessing Methods\n",
      "============================================================\n",
      "Testing: HOG features + SVM classifier\n",
      "\n",
      "  Simple              : 0.9417\n",
      "  Blur                : 0.9558\n",
      "  Histogram Eq        : 0.9475\n",
      "  Advanced            : 0.9442\n",
      "\n",
      "Preprocessing  Accuracy\n",
      "       Simple  0.941667\n",
      "         Blur  0.955833\n",
      " Histogram Eq  0.947500\n",
      "     Advanced  0.944167\n",
      "\n",
      "✓ Best preprocessing: Blur (0.9558)\n"
     ]
    }
   ],
   "source": [
    "# Exploration 1: Compare Preprocessing Methods\n",
    "print(\"=\" * 60)\n",
    "print(\"T4.1: Compare Preprocessing Methods\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing: HOG features + SVM classifier\\n\")\n",
    "\n",
    "# Test different preprocessing methods\n",
    "preprocessing_methods = {\n",
    "    'Simple': preprocessing_simple,\n",
    "    'Blur': preprocessing_blur,\n",
    "    'Histogram Eq': preprocessing_histogram_eq,\n",
    "    'Advanced': preprocessing_advanced,\n",
    "}\n",
    "\n",
    "results_preprocessing = []\n",
    "classifier = SVC()\n",
    "\n",
    "for prep_name, prep_func in preprocessing_methods.items():\n",
    "    # Preprocess\n",
    "    X_processed = prep_func(X)\n",
    "    \n",
    "    # Extract features\n",
    "    X_features = feature_HOG(X_processed)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_features, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    \n",
    "    results_preprocessing.append({\n",
    "        'Preprocessing': prep_name,\n",
    "        'Accuracy': accuracy\n",
    "    })\n",
    "    print(f\"  {prep_name:20s}: {accuracy:.4f}\")\n",
    "\n",
    "df_preprocessing = pd.DataFrame(results_preprocessing)\n",
    "print(\"\\n\" + df_preprocessing.to_string(index=False))\n",
    "\n",
    "# Find best preprocessing\n",
    "best_prep = df_preprocessing.loc[df_preprocessing['Accuracy'].idxmax()]\n",
    "print(f\"\\n✓ Best preprocessing: {best_prep['Preprocessing']} ({best_prep['Accuracy']:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4.2: Compare Classifiers\n",
    "\n",
    "**Goal**: Compare classifier performance.\n",
    "\n",
    "**Setup**: Blur preprocessing + HOG features, vary classifier type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "T4.2: Compare Classifiers\n",
      "============================================================\n",
      "Testing: Blur preprocessing + HOG features\n",
      "\n",
      "✓ Preprocessed: 5998 images\n",
      "✓ Features extracted: (5998, 128)\n",
      "  SVM                 : 0.9558\n",
      "  Random Forest       : 0.9750\n",
      "  kNN                 : 0.8633\n",
      "  Decision Tree       : 0.9250\n",
      "  Naive Bayes         : 0.7992\n",
      "  MLP                 : 0.9667\n",
      "\n",
      "   Classifier  Accuracy\n",
      "          SVM  0.955833\n",
      "Random Forest  0.975000\n",
      "          kNN  0.863333\n",
      "Decision Tree  0.925000\n",
      "  Naive Bayes  0.799167\n",
      "          MLP  0.966667\n",
      "\n",
      "✓ Best classifier: Random Forest (0.9750)\n"
     ]
    }
   ],
   "source": [
    "# Exploration 2: Compare Classifiers\n",
    "print(\"=\" * 60)\n",
    "print(\"T4.2: Compare Classifiers\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing: Blur preprocessing + HOG features\\n\")\n",
    "\n",
    "# Preprocess (using blur from baseline)\n",
    "X_processed = preprocessing_blur(X)\n",
    "print(f\"✓ Preprocessed: {len(X_processed)} images\")\n",
    "\n",
    "# Extract features\n",
    "X_features = feature_HOG(X_processed)\n",
    "print(f\"✓ Features extracted: {X_features.shape}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'SVM': SVC(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'kNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'MLP': MLPClassifier(max_iter=500)\n",
    "}\n",
    "\n",
    "results_classifiers = []\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    results_classifiers.append({'Classifier': name, 'Accuracy': accuracy})\n",
    "    print(f\"  {name:20s}: {accuracy:.4f}\")\n",
    "\n",
    "df_classifiers = pd.DataFrame(results_classifiers)\n",
    "print(\"\\n\" + df_classifiers.to_string(index=False))\n",
    "\n",
    "# Find best classifier\n",
    "best_classifier = df_classifiers.loc[df_classifiers['Accuracy'].idxmax()]\n",
    "print(f\"\\n✓ Best classifier: {best_classifier['Classifier']} ({best_classifier['Accuracy']:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration 3: Advanced Techniques & Combinations\n",
    "\n",
    "**Goal**: Explore advanced techniques and find best combinations.\n",
    "\n",
    "### Part A: Multi-feature + PCA + Scaling\n",
    "\n",
    "**PCA**: Reduces dimensions while preserving information. Finds principal components (directions of max variance), projects data, keeps top N components.\n",
    "\n",
    "**Why Scaling**: Features have different scales (HOG: 0-1, area: 0-2304). Classifiers (SVM, kNN, MLP) are scale-sensitive. StandardScaler normalizes to mean=0, std=1.\n",
    "\n",
    "**Setup**: Advanced preprocessing + Multi-feature (HOG+Color+Shape) + PCA + Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "T4.3: Advanced Techniques (Multi-feature + PCA + Scaling)\n",
      "============================================================\n",
      "✓ Preprocessed: 5998 images\n",
      "✓ Features extracted: (5998, 155) (before PCA)\n",
      "✓ After PCA: (5998, 100) (reduced from 155 to 100)\n",
      "  SVM                 : 0.9600\n",
      "  Random Forest       : 0.9500\n",
      "  kNN                 : 0.7950\n",
      "  Decision Tree       : 0.8950\n",
      "  Naive Bayes         : 0.7575\n",
      "  MLP                 : 0.9617\n",
      "\n",
      "   Classifier  Accuracy\n",
      "          SVM  0.960000\n",
      "Random Forest  0.950000\n",
      "          kNN  0.795000\n",
      "Decision Tree  0.895000\n",
      "  Naive Bayes  0.757500\n",
      "          MLP  0.961667\n"
     ]
    }
   ],
   "source": [
    "# Exploration 3A: Multi-feature + PCA + Scaling\n",
    "print(\"=\" * 60)\n",
    "print(\"T4.3: Advanced Techniques (Multi-feature + PCA + Scaling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Preprocess\n",
    "X_processed = preprocessing_advanced(X)\n",
    "print(f\"✓ Preprocessed: {len(X_processed)} images\")\n",
    "\n",
    "# Extract multi-features\n",
    "X_features = feature_multi(X, X_processed)\n",
    "print(f\"✓ Features extracted: {X_features.shape} (before PCA)\")\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=100)\n",
    "X_features = pca.fit_transform(X_features)\n",
    "print(f\"✓ After PCA: {X_features.shape} (reduced from {feature_multi(X[:1], X_processed[:1]).shape[1]} to 100)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Test classifiers with scaling\n",
    "results_advanced = []\n",
    "for name, classifier in classifiers.items():\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    classifier.fit(X_train_scaled, y_train)\n",
    "    accuracy = classifier.score(X_test_scaled, y_test)\n",
    "    results_advanced.append({'Classifier': name, 'Accuracy': accuracy})\n",
    "    print(f\"  {name:20s}: {accuracy:.4f}\")\n",
    "\n",
    "df_advanced = pd.DataFrame(results_advanced)\n",
    "print(\"\\n\" + df_advanced.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: All Combinations Comparison\n",
    "\n",
    "**Goal**: Test all combinations of preprocessing × feature × classifier to find optimal setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "T3.1: Compare Feature Extraction Methods\n",
      "============================================================\n",
      "Testing: preprocessing × feature extraction × classifier\n",
      "\n",
      "  Simple          × HOG            : 0.9417\n",
      "  Simple          × LBP            : 0.9158\n",
      "  Simple          × Pyramid        : 0.8175\n",
      "  Simple          × FFT            : 0.7950\n",
      "  Simple          × Hu Moments     : 0.0858\n",
      "  Blur            × HOG            : 0.9558\n",
      "  Blur            × LBP            : 0.9275\n",
      "  Blur            × Pyramid        : 0.7533\n",
      "  Blur            × FFT            : 0.5825\n",
      "  Blur            × Hu Moments     : 0.0875\n",
      "  Histogram Eq    × HOG            : 0.9475\n",
      "  Histogram Eq    × LBP            : 0.9075\n",
      "  Histogram Eq    × Pyramid        : 0.8775\n",
      "  Histogram Eq    × FFT            : 0.8283\n",
      "  Histogram Eq    × Hu Moments     : 0.0850\n",
      "\n",
      "Preprocessing    Feature  Accuracy\n",
      "       Simple        HOG  0.941667\n",
      "       Simple        LBP  0.915833\n",
      "       Simple    Pyramid  0.817500\n",
      "       Simple        FFT  0.795000\n",
      "       Simple Hu Moments  0.085833\n",
      "         Blur        HOG  0.955833\n",
      "         Blur        LBP  0.927500\n",
      "         Blur    Pyramid  0.753333\n",
      "         Blur        FFT  0.582500\n",
      "         Blur Hu Moments  0.087500\n",
      " Histogram Eq        HOG  0.947500\n",
      " Histogram Eq        LBP  0.907500\n",
      " Histogram Eq    Pyramid  0.877500\n",
      " Histogram Eq        FFT  0.828333\n",
      " Histogram Eq Hu Moments  0.085000\n",
      "\n",
      "✓ Best combination: Blur × HOG = 0.9558\n"
     ]
    }
   ],
   "source": [
    "# Exploration 3B: All Combinations\n",
    "print(\"=\" * 60)\n",
    "print(\"T3.1: Compare Feature Extraction Methods\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Testing: preprocessing × feature extraction × classifier\\n\")\n",
    "\n",
    "# Define methods (subset for speed - can expand)\n",
    "preprocessing_methods = {\n",
    "    'Simple': preprocessing_simple,\n",
    "    'Blur': preprocessing_blur,\n",
    "    'Histogram Eq': preprocessing_histogram_eq,\n",
    "}\n",
    "\n",
    "feature_methods = {\n",
    "    'HOG': feature_HOG,\n",
    "    'LBP': feature_LBP,\n",
    "    'Pyramid': feature_pyramid,\n",
    "    'FFT': feature_FFT,\n",
    "    'Hu Moments': feature_HuMoments,\n",
    "}\n",
    "\n",
    "# Test combinations (using SVM for speed - can test all classifiers)\n",
    "results_combinations = []\n",
    "classifier = SVC()\n",
    "\n",
    "for prep_name, prep_func in preprocessing_methods.items():\n",
    "    X_processed = prep_func(X)\n",
    "    \n",
    "    for feat_name, feat_func in feature_methods.items():\n",
    "        try:\n",
    "            X_features = feat_func(X_processed)\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_features, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            classifier.fit(X_train, y_train)\n",
    "            accuracy = classifier.score(X_test, y_test)\n",
    "            \n",
    "            results_combinations.append({\n",
    "                'Preprocessing': prep_name,\n",
    "                'Feature': feat_name,\n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "            print(f\"  {prep_name:15s} × {feat_name:15s}: {accuracy:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {prep_name:15s} × {feat_name:15s}: ERROR - {str(e)[:30]}\")\n",
    "\n",
    "df_combinations = pd.DataFrame(results_combinations)\n",
    "if len(df_combinations) > 0:\n",
    "    print(\"\\n\" + df_combinations.to_string(index=False))\n",
    "    \n",
    "    # Find best combination\n",
    "    best = df_combinations.loc[df_combinations['Accuracy'].idxmax()]\n",
    "    print(f\"\\n✓ Best combination: {best['Preprocessing']} × {best['Feature']} = {best['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUMMARY & CONCLUSIONS\n",
      "============================================================\n",
      "\n",
      "1. Preprocessing Methods Comparison:\n",
      "Preprocessing  Accuracy\n",
      "       Simple  0.941667\n",
      "         Blur  0.955833\n",
      " Histogram Eq  0.947500\n",
      "     Advanced  0.944167\n",
      "\n",
      "2. Classifiers Comparison:\n",
      "   Classifier  Accuracy\n",
      "          SVM  0.955833\n",
      "Random Forest  0.975000\n",
      "          kNN  0.863333\n",
      "Decision Tree  0.925000\n",
      "  Naive Bayes  0.799167\n",
      "          MLP  0.966667\n",
      "\n",
      "3. Advanced Approach (Multi-feature + PCA + Scaling):\n",
      "   Classifier  Accuracy\n",
      "          SVM  0.960000\n",
      "Random Forest  0.950000\n",
      "          kNN  0.795000\n",
      "Decision Tree  0.895000\n",
      "  Naive Bayes  0.757500\n",
      "          MLP  0.961667\n",
      "\n",
      "4. Best Combinations (Preprocessing × Feature):\n",
      "Preprocessing Feature  Accuracy\n",
      "         Blur     HOG  0.955833\n",
      " Histogram Eq     HOG  0.947500\n",
      "       Simple     HOG  0.941667\n",
      "         Blur     LBP  0.927500\n",
      "       Simple     LBP  0.915833\n",
      "\n",
      "============================================================\n",
      "BEST RESULTS:\n",
      "============================================================\n",
      "Best Preprocessing:        Blur                 (0.9558)\n",
      "Best Classifier:           Random Forest        (0.9750)\n",
      "Advanced Approach:         Multi-feature+PCA+Scaling (0.9617)\n",
      "Best Combination:         Blur × HOG                  (0.9558)\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHTS:\n",
      "============================================================\n",
      "✓ Single features perform well\n",
      "✓ Advanced approach (multi-feature + PCA) is optimal\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY & CONCLUSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Preprocessing Methods Comparison:\")\n",
    "print(df_preprocessing.to_string(index=False))\n",
    "\n",
    "print(\"\\n2. Classifiers Comparison:\")\n",
    "print(df_classifiers.to_string(index=False))\n",
    "\n",
    "print(\"\\n3. Advanced Approach (Multi-feature + PCA + Scaling):\")\n",
    "print(df_advanced.to_string(index=False))\n",
    "\n",
    "if len(results_combinations) > 0:\n",
    "    print(\"\\n4. Best Combinations (Preprocessing × Feature):\")\n",
    "    print(df_combinations.nlargest(5, 'Accuracy').to_string(index=False))\n",
    "\n",
    "# Compare best accuracies\n",
    "best_prep = df_preprocessing['Accuracy'].max()\n",
    "best_classifier = df_classifiers['Accuracy'].max()\n",
    "best_advanced = df_advanced['Accuracy'].max()\n",
    "best_combination = df_combinations['Accuracy'].max() if len(results_combinations) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST RESULTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Preprocessing:        {df_preprocessing.loc[df_preprocessing['Accuracy'].idxmax(), 'Preprocessing']:20s} ({best_prep:.4f})\")\n",
    "print(f\"Best Classifier:           {df_classifiers.loc[df_classifiers['Accuracy'].idxmax(), 'Classifier']:20s} ({best_classifier:.4f})\")\n",
    "print(f\"Advanced Approach:         Multi-feature+PCA+Scaling ({best_advanced:.4f})\")\n",
    "if best_combination > 0:\n",
    "    best_combo = df_combinations.loc[df_combinations['Accuracy'].idxmax()]\n",
    "    print(f\"Best Combination:         {best_combo['Preprocessing']} × {best_combo['Feature']:20s} ({best_combination:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\" * 60)\n",
    "if best_advanced > best_classifier:\n",
    "    improvement = (best_advanced - best_classifier) * 100\n",
    "    print(f\"✓ Multi-feature + PCA improves by {improvement:.2f}% over single features\")\n",
    "else:\n",
    "    print(\"✓ Single features perform well\")\n",
    "\n",
    "if best_combination > 0 and best_combination > best_advanced:\n",
    "    print(f\"✓ Best combination outperforms advanced approach\")\n",
    "    print(f\"  → Use: {df_combinations.loc[df_combinations['Accuracy'].idxmax(), 'Preprocessing']} + {df_combinations.loc[df_combinations['Accuracy'].idxmax(), 'Feature']}\")\n",
    "else:\n",
    "    print(\"✓ Advanced approach (multi-feature + PCA) is optimal\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
